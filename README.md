# Data-Analyst-4
python清洗与分析数据-Twitter狗狗

**简介**

现实世界的数据通常都不干净。使用 Python 以及 Python 的库，你可以收集各种来源、各种格式的数据，评估数据的质量和整洁度，然后进行清洗。这个过程叫做数据整理。你可以在 Jupyter Notebook 中记录并展示数据整理的过程，然后使用 Python (及其库) 和/或 SQL 进行分析和可视化。

你将要整理 (以及分析和可视化) 的数据集是推特用户 @dog_rates 的档案, 推特昵称为 WeRateDogs。WeRateDogs 是一个推特主，他以诙谐幽默的方式对人们的宠物狗评分。这些评分通常以 10 作为分母。但是分子则一般大于 10：11/10、12/10、13/10 等等。为什么会有这样的评分？因为 "They're good dogs Brent." WeRateDogs 拥有四百多万关注者，曾受到国际媒体的报道。

WeRateDogs 下载了他们的推特档案，并通过电子邮件发送给优达学城，专门为本项目使用。这个档案是基本的推特数据（推特 ID、时间戳、推特文本等），包含了截止到 2017 年 4 月 1 日的 5000 多条推特。

**项目动机**

**背景**

你的目标：清洗 WeRateDogs 推特数据，创建有趣且可靠的分析和可视化。这份推特档案很棒，但是只包含基本的推特信息。要达到 "Wow!" 的效果，在分析和可视化前，还需要收集额外的数据、然后进行评估和清洗。

**数据**

**完善推特档案**

WeRateDogs 的推特档案包括 5000 多条推特的基本信息，但并不包括所有内容。不过档案中有一列包含每个推特的文本，我用这一列数据提取了评分、狗的名字和“地位”（即 doggo、floofer、pupper 和 puppo）——这使数据得以“完善”。在这 5000 多条中，我只筛选出了 2356 条包含评分的推特数据。

**通过推特 API 获取附加数据**

回到基础的推特档案：转发数（retweet count）和喜爱数（favorite count）是两个遗漏的列。幸运的是，从推特 API 中，任何人都可以收集到这些数据。其实，"任何人" 只是能获取最多 3000 条的最近推特数据。但是因为你拥有 WeRateDogs 推特档案和其中的推特 ID，你可以收集到这其中所有的 5000 多条推特。你将查询推特 API 来收集这些有价值的数据。

特别提示： 如果你无法访问 Twitter 的话，我们将直接给你提供返回的 Twitter 数据。你可以 右键点击这里选择另存为 下载。该文件为 txt 格式，每一行为一条独立的 twitter 信息，格式为 JSON。该文件比较大，下载可能需要几分钟。

**图像预测文件**

还有一件更酷的事情：我通过一个可以对狗狗种类进行分类的神经网络，运行这份推特档案中的所有图像。获取的结果：一份图像预测结果表格，其中包含了预测结果的前三名，推特 ID，图像 url 以及最可信的预测结果对应的图像编号（由于推特最多包含 4 个图片，所以编号为 1 到 4）。

以该表格中的最后一行数据来理解各列数据：

tweet_id 是推特链接的最后一部分，位于 "status/" 后面 → https://twitter.com/dog_rates/status/889531135344209921

jpg_url 是预测的图像资源链接

img_num 最可信的预测结果对应的图像编号 → 1 推特中的第一张图片

p1 是算法对推特中图片的一号预测 → 金毛犬

p1_conf 是算法的一号预测的可信度 → 95%

p1_dog 是一号预测该图片是否属于“狗”（有可能是其他物种，比如熊、马等） → True 真

p2 是算法对推特中图片预测的第二种可能性 → 拉布拉多犬

p2_conf 是算法的二号预测的可信度 → 1%

p2_dog 是二号预测该图片是否属于“狗” → True 真

以此类推...


**文件信息**

wrangle_act.ipynb：用于收集、评估、清洗、分析和可视化数据的代码

wrangle_report.pdf：汇总数据整理步骤的文档：收集，评估和清洗

act_report.pdf：对最终数据进行观察与分析的文档

twitter_archive_enhanced.csv：给定的文件

image_predictions.tsv：以编程方式下载的文件

tweet_json.txt：通过 API 构建的文件

twitter_archive_master.csv：合并与清洗后的数据

